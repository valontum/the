{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site successfuly stored!\n",
      "a\n",
      "{'class': re.compile('hp_mob_text_ff')}\n",
      "Downloaded file: 0 of 10\n",
      "a\n",
      "{'class': re.compile('hp_mob_text_ff')}\n",
      "Downloaded file: 1 of 10\n",
      "a\n",
      "{'class': re.compile('hp_mob_text_ff')}\n",
      "Downloaded file: 2 of 10\n",
      "a\n",
      "{'class': re.compile('hp_mob_text_ff')}\n",
      "Downloaded file: 3 of 10\n",
      "a\n",
      "{'class': re.compile('hp_mob_text_ff')}\n",
      "Downloaded file: 4 of 10\n",
      "a\n",
      "{'class': re.compile('hp_mob_text_ff')}\n",
      "Downloaded file: 5 of 10\n",
      "a\n",
      "{'class': re.compile('hp_mob_text_ff')}\n",
      "Downloaded file: 6 of 10\n",
      "a\n",
      "{'class': re.compile('hp_mob_text_ff')}\n",
      "Downloaded file: 7 of 10\n",
      "a\n",
      "{'class': re.compile('hp_mob_text_ff')}\n",
      "Downloaded file: 8 of 10\n",
      "a\n",
      "{'class': re.compile('hp_mob_text_ff')}\n",
      "Downloaded file: 9 of 10\n",
      "a\n",
      "{'class': re.compile('hp_mob_text_ff')}\n",
      "Downloaded file: 10 of 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import urllib.request\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "# pprint library is used to make the output look more pretty\n",
    "\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db=client.articles\n",
    "# Issue the serverStatus command and print the results\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "articleUrls = [] #stores article urls\n",
    "articleUrlsToCrawl = [] #stores article urls\n",
    "articles = [] #stores articles\n",
    "sources = [] #stores site patterns and other infos\n",
    "\n",
    "\n",
    "\n",
    "def saveUrl(obj):\n",
    "    \n",
    "     \n",
    "     db.urls.insert_one(obj)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getArticlesUrl(file, source, rootUrl):\n",
    "    import re\n",
    "   \n",
    "    \n",
    "    soup = BeautifulSoup(file, \"lxml\")\n",
    "        \n",
    "  \n",
    "    exec('items = ' + source[\"urlPattern\"],locals())\n",
    "     \n",
    "    \n",
    "    \n",
    "    for item in locals()[\"items\"]:\n",
    "        \n",
    "            \n",
    "       \n",
    "        print(item)\n",
    "       \n",
    "       \n",
    "\n",
    "\n",
    "def crawlPageTypeOpinionsUrlList(source):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(0, source[\"size\"]+1):      \n",
    "\n",
    "        try:\n",
    "            path = source[\"url\"]+str(i)\n",
    "\n",
    "            r = urllib.request.urlopen(path).read()\n",
    "\n",
    "            getArticlesUrl(r, source, source[\"rootUrl\"])    \n",
    "\n",
    "            print (\"Downloaded file: \"+str(i) + \" of \"+str(source[\"size\"]))\n",
    "\n",
    "        except Exception as e: \n",
    "            \n",
    "            print(e)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def insertSiteIntoDB(obj):\n",
    "\n",
    "    source = obj\n",
    "    try:\n",
    "        if db.sites.find({\"rootUrl\":obj[\"rootUrl\"]}).count() == 0:\n",
    "            db.sites.insert_one(obj)\n",
    "            print(\"Site successfuly stored!\")\n",
    "            crawlPageTypeOpinionsUrlList(source)\n",
    "        \n",
    "        else:\n",
    "            print(\"Site already exists\")\n",
    "        \n",
    "    except ValueError:\n",
    "        print(\"Oops!  Something worng, Please try it once more\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def readNewSiteFromConsole():\n",
    "    \n",
    " \n",
    "    \n",
    "    source = {\"rootUrl\":\"https://www.972mag.com\",\"size\":10,\"url\":\"https://972mag.com/c/analysis/page/\",\"urlPattern\":\"soup.find_all('a', {'class':re.compile(r'hp_mob_text_ff')})\"}\n",
    "    \n",
    "    \n",
    "    insertSiteIntoDB(source)\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "def testQueries():\n",
    "    \n",
    "    url = input(\"url: \")\n",
    "    print(\"Press 'x' for new url\")\n",
    "    while(True):\n",
    "        \n",
    "        query = input(\"query: \")\n",
    "        if(query==\"x\" or query == \"X\"):\n",
    "            url = input(\"url: \")\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            try:\n",
    "                r = urllib.request.urlopen(url).read()\n",
    "\n",
    "\n",
    "\n",
    "                import re\n",
    "\n",
    "\n",
    "                soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "\n",
    "                exec('items = ' + query,locals())\n",
    "\n",
    "                print(locals()[\"items\"])\n",
    "\n",
    "            except Exception as e: \n",
    "            \n",
    "                print(e)\n",
    "    \n",
    "                 \n",
    "\n",
    "            \n",
    "                    \n",
    "\n",
    "   \n",
    "readNewSiteFromConsole()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    authorPattern = input(\"authorPattern: \") \n",
    "    bodyPattern = input(\"bodyPattern: \")\n",
    "    category = input(\"category: \")\n",
    "    datePattern = input(\"datePattern: \")\n",
    "    rootUrl = input(\"rootUrl: \")\n",
    "    size = int(input(\"size: \"))\n",
    "    titlePattern = input(\"titlePattern: \")\n",
    "    url = input(\"url: \")\n",
    "    urlPattern = input(\"urlPattern: \"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "command = input(\"'t' for test, 'r' for new site entry\")\n",
    "\n",
    "if(command == 't'):\n",
    "    testQueries()\n",
    "else:\n",
    "    readNewSiteFromConsole()    \n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
